print("üîß CUSTOMIZATION GUIDE FOR YOUR RESEARCH")
print("=" * 50)

print("""
üìã STEP 1: Prepare Your Dataset
   Replace the load_data() function with:
   
   def load_data():
       df = pd.read_csv('your_dataset.csv')
       df['timestamp'] = pd.to_datetime(df['timestamp'])
       return df

üìä STEP 2: Add Real External Indicators
   In CounterfeitDetector class, replace simulated indicators:
   
   ‚Ä¢ seller_takedown_history: Use real takedown data
   ‚Ä¢ price_analysis: Compare with market prices
   ‚Ä¢ image_analysis: Use computer vision for image authenticity
   ‚Ä¢ legal_actions: Include known IP infringement cases

üéØ STEP 3: Customize Detection Heuristics
   Adjust weights in FakeReviewLabeler based on your domain:
   
   fake_labeler.fake_indicators['short_review_high_rating']['weight'] = 0.4
   fake_labeler.fake_indicators['review_burst']['weight'] = 0.6

ü§ñ STEP 4: Add Advanced Models
   Include transformer-based models for better performance:
   
   ‚Ä¢ BERT for text classification
   ‚Ä¢ RoBERTa for sentiment analysis
   ‚Ä¢ DistilBERT for efficiency

üìà STEP 5: Validation with Ground Truth
   If you have labeled data:
   
   ‚Ä¢ Compare predictions with actual fake/counterfeit labels
   ‚Ä¢ Calculate precision, recall, F1-score
   ‚Ä¢ Perform statistical significance tests

üîç STEP 6: Domain-Specific Features
   Add features relevant to your specific e-commerce platform:
   
   ‚Ä¢ Platform-specific metadata
   ‚Ä¢ Category-specific indicators
   ‚Ä¢ Regional/language patterns
   ‚Ä¢ Seasonal trends

üìä STEP 7: Scaling for Large Datasets
   For datasets > 100K reviews:
   
   ‚Ä¢ Use batch processing
   ‚Ä¢ Implement parallel processing
   ‚Ä¢ Consider distributed computing (Dask/Spark)
   ‚Ä¢ Optimize memory usage
""")

print("\nüéì RESEARCH APPLICATIONS:")
print("""
üìö For Academic Research:
   ‚Ä¢ Use correlation analysis for hypothesis testing
   ‚Ä¢ Apply statistical tests for significance
   ‚Ä¢ Create visualizations for publications
   ‚Ä¢ Generate reproducible results

üè¢ For Industry Applications:
   ‚Ä¢ Deploy models for real-time detection
   ‚Ä¢ Create monitoring dashboards
   ‚Ä¢ Implement alert systems
   ‚Ä¢ Integrate with existing platforms

üìã For Policy Research:
   ‚Ä¢ Generate evidence for regulatory decisions
   ‚Ä¢ Analyze market-wide patterns
   ‚Ä¢ Support IP enforcement actions
   ‚Ä¢ Inform platform policies
""")

print("=" * 50)## 6. Next Steps and Customization

**üîß How to Use This Pipeline with Your Real Data**

To adapt this pipeline for your actual research data:# Generate research insights
print("üéì RESEARCH INSIGHTS FOR DISSERTATION")
print("=" * 60)

print("\nüìä 1. FAKE REVIEW CHARACTERISTICS:")
if 'fake_probability' in df_final.columns:
    high_fake_prob = df_final[df_final['fake_probability'] > 0.7]
    print(f"   ‚Ä¢ {len(high_fake_prob)} reviews ({len(high_fake_prob)/len(df_final)*100:.1f}%) have high fake probability (>0.7)")
    
    if len(high_fake_prob) > 0:
        print(f"   ‚Ä¢ Average rating of high-fake-probability reviews: {high_fake_prob['rating'].mean():.2f}")
        print(f"   ‚Ä¢ Average word count: {high_fake_prob['word_count'].mean():.1f}")
        if 'vader_compound' in high_fake_prob.columns:
            print(f"   ‚Ä¢ Average sentiment: {high_fake_prob['vader_compound'].mean():.3f}")

print("\nüè¥‚Äç‚ò†Ô∏è 2. COUNTERFEIT PRODUCT PATTERNS:")
if 'counterfeit_probability' in df_final.columns:
    high_counterfeit_prob = df_final[df_final['counterfeit_probability'] > 0.6]
    print(f"   ‚Ä¢ {len(high_counterfeit_prob)} products ({len(high_counterfeit_prob)/len(df_final)*100:.1f}%) have high counterfeit probability (>0.6)")
    
    if len(high_counterfeit_prob) > 0 and 'fake_probability' in high_counterfeit_prob.columns:
        print(f"   ‚Ä¢ Average fake review probability for suspected counterfeits: {high_counterfeit_prob['fake_probability'].mean():.3f}")

print("\nüîó 3. CORRELATION FINDINGS:")
if correlation_results and 'pearson' in correlation_results:
    pearson = correlation_results['pearson']
    if 'fake_probability' in pearson.columns and 'counterfeit_probability' in pearson.index:
        corr_coef = pearson.loc['fake_probability', 'counterfeit_probability']
        print(f"   ‚Ä¢ Fake reviews and counterfeit products correlation: {corr_coef:.3f}")
        
        if abs(corr_coef) > 0.3:
            strength = "strong" if abs(corr_coef) > 0.7 else "moderate"
            direction = "positive" if corr_coef > 0 else "negative"
            print(f"   ‚Ä¢ This indicates a {strength} {direction} relationship")

print("\nü§ñ 4. MODEL PERFORMANCE:")
best_fake_model = None
best_fake_auc = 0
for model_name, results in fake_models.items():
    auc = results.get('roc_auc', 0)
    if auc > best_fake_auc:
        best_fake_auc = auc
        best_fake_model = model_name

if best_fake_model:
    print(f"   ‚Ä¢ Best performing model for fake review detection: {best_fake_model.replace('_', ' ').title()}")
    print(f"   ‚Ä¢ ROC-AUC Score: {best_fake_auc:.3f}")
    
    if best_fake_auc > 0.8:
        print("   ‚Ä¢ Model shows excellent discriminative ability")
    elif best_fake_auc > 0.7:
        print("   ‚Ä¢ Model shows good discriminative ability")
    else:
        print("   ‚Ä¢ Model shows moderate discriminative ability")

print("\nüìà 5. BUSINESS IMPLICATIONS:")
if 'is_fake_predicted' in df_final.columns and 'is_counterfeit_predicted' in df_final.columns:
    both_flags = df_final[(df_final['is_fake_predicted'] == 1) & (df_final['is_counterfeit_predicted'] == 1)]
    print(f"   ‚Ä¢ {len(both_flags)} reviews ({len(both_flags)/len(df_final)*100:.1f}%) are flagged as both fake and from counterfeit products")
    print("   ‚Ä¢ This suggests a strong operational link between review manipulation and IP infringement")

print("\nüéØ 6. RESEARCH CONTRIBUTIONS:")
print("   ‚Ä¢ Developed a comprehensive framework for detecting fake reviews")
print("   ‚Ä¢ Established quantitative link between fake reviews and counterfeit products")
print("   ‚Ä¢ Created predictive models for identifying high-risk sellers/products")
print("   ‚Ä¢ Provided evidence-based insights for platform policy and enforcement")

print("\n" + "=" * 60)## 5. Research Insights and Conclusions

**üéì Key Findings for Your Dissertation**

Based on the analysis, here are the key insights you can use in your research:# Analyze correlations and relationships
print("üîó Correlation Analysis:")
print("=" * 40)

correlation_results = results.get('correlation_results', {})
relationship_analysis = results.get('relationship_analysis', {})

if 'pearson' in correlation_results:
    pearson_corr = correlation_results['pearson']
    
    # Key correlations
    if 'fake_probability' in pearson_corr.columns and 'counterfeit_probability' in pearson_corr.index:
        fake_counterfeit_corr = pearson_corr.loc['fake_probability', 'counterfeit_probability']
        print(f"üìä Fake Reviews ‚Üî Counterfeit Products: {fake_counterfeit_corr:.3f}")
    
    # Other interesting correlations
    interesting_pairs = [
        ('fake_probability', 'rating'),
        ('counterfeit_probability', 'rating'),
        ('vader_compound', 'rating'),
        ('word_count', 'fake_probability')
    ]
    
    print("\nüìà Other Key Correlations:")
    for var1, var2 in interesting_pairs:
        if var1 in pearson_corr.columns and var2 in pearson_corr.index:
            corr_value = pearson_corr.loc[var2, var1]
            print(f"   {var1} ‚Üî {var2}: {corr_value:.3f}")
        elif var2 in pearson_corr.columns and var1 in pearson_corr.index:
            corr_value = pearson_corr.loc[var1, var2]
            print(f"   {var1} ‚Üî {var2}: {corr_value:.3f}")

# Statistical significance
if relationship_analysis:
    p_value = relationship_analysis.get('p_value')
    is_significant = relationship_analysis.get('is_significant')
    
    if p_value is not None:
        print(f"\nüìä Statistical Significance:")
        print(f"   p-value: {p_value:.4f}")
        print(f"   Significant (p < 0.05): {'Yes' if is_significant else 'No'}")

print("\n" + "=" * 40)# Analyze model performance
print("ü§ñ Model Performance Summary:")
print("=" * 50)

fake_models = results.get('fake_model_results', {})
counterfeit_models = results.get('counterfeit_model_results', {})

print("\nüîç Fake Review Detection Models:")
for model_name, model_results in fake_models.items():
    roc_auc = model_results.get('roc_auc', 0)
    report = model_results.get('classification_report', {})
    accuracy = report.get('accuracy', 0) if report else 0
    
    print(f"   {model_name.replace('_', ' ').title()}:")
    print(f"      ROC-AUC: {roc_auc:.3f}")
    print(f"      Accuracy: {accuracy:.3f}")
    
    if 'weighted avg' in report:
        weighted_avg = report['weighted avg']
        print(f"      Precision: {weighted_avg.get('precision', 0):.3f}")
        print(f"      Recall: {weighted_avg.get('recall', 0):.3f}")
        print(f"      F1-Score: {weighted_avg.get('f1-score', 0):.3f}")

print("\nüè¥‚Äç‚ò†Ô∏è Counterfeit Detection Models:")
for model_name, model_results in counterfeit_models.items():
    roc_auc = model_results.get('roc_auc', 0)
    report = model_results.get('classification_report', {})
    accuracy = report.get('accuracy', 0) if report else 0
    
    print(f"   {model_name.replace('_', ' ').title()}:")
    print(f"      ROC-AUC: {roc_auc:.3f}")
    print(f"      Accuracy: {accuracy:.3f}")

print("\n" + "=" * 50)# Access the processed dataframe
df_final = results['dataframe']

print("üìä Final Dataset Overview:")
print(f"   Shape: {df_final.shape}")
print(f"   Columns: {len(df_final.columns)}")
print(f"   Memory usage: {df_final.memory_usage(deep=True).sum() / 1024**2:.1f} MB")

print("\nüîç Key Features Generated:")
feature_categories = {
    'Text Features': [col for col in df_final.columns if any(x in col for x in ['word_count', 'char_count', 'lexical'])],
    'Sentiment Features': [col for col in df_final.columns if any(x in col for x in ['vader', 'emotion', 'sentiment'])],
    'Temporal Features': [col for col in df_final.columns if any(x in col for x in ['hour', 'day_of_week', 'frequency'])],
    'Fake Review Features': [col for col in df_final.columns if 'fake' in col],
    'Counterfeit Features': [col for col in df_final.columns if 'counterfeit' in col]
}

for category, features in feature_categories.items():
    print(f"   {category}: {len(features)} features")
    if features:
        print(f"      Examples: {features[:3]}")

# Show sample of final data
print("\nüìã Sample of Processed Data:")
display_columns = ['review_text', 'rating', 'fake_probability', 'counterfeit_probability', 
                  'is_fake_predicted', 'is_counterfeit_predicted']
available_display_cols = [col for col in display_columns if col in df_final.columns]
print(df_final[available_display_cols].head())## 4. Detailed Results Analysis

**üìä Examine the Results in Detail**

Let's explore the comprehensive results generated by the pipeline:# Import the complete pipeline module
from fake_review_detection import run_complete_pipeline

# Run the complete pipeline with synthetic data
print("üöÄ Starting Complete Pipeline Analysis...")
print("This may take a few minutes to complete all steps...")

# Execute the pipeline
results = run_complete_pipeline(use_synthetic_data=True)

print("\n‚úÖ Pipeline execution completed!")
print("\nResults structure:")
for key, value in results.items():
    if isinstance(value, dict):
        print(f"üìã {key}: {len(value)} items")
    else:
        print(f"üìã {key}: {type(value).__name__}")## 3. Complete Pipeline Execution

**üöÄ Run the Complete Analysis Pipeline**

This section demonstrates how to use the modular pipeline for comprehensive analysis. The pipeline includes all components: text preprocessing, sentiment analysis, fake review detection, counterfeit detection, feature engineering, model training, and visualization.class TextPreprocessor:
    """Comprehensive text preprocessing pipeline"""
    
    def __init__(self, nlp_model=None):
        self.nlp = nlp_model
        self.sia = SentimentIntensityAnalyzer()
    
    def clean_text(self, text):
        """Clean and normalize text"""
        if pd.isna(text):
            return ""
        
        # Convert to string and lowercase
        text = str(text).lower()
        
        # Remove HTML tags
        text = re.sub(r'<[^>]+>', '', text)
        
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        
        # Remove email addresses
        text = re.sub(r'\S+@\S+', '', text)
        
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        
        # Remove leading/trailing whitespace
        text = text.strip()
        
        return text
    
    def extract_text_features(self, text):
        """Extract linguistic features from text"""
        if pd.isna(text) or text == "":
            return {
                'word_count': 0,
                'char_count': 0,
                'sentence_count': 0,
                'avg_word_length': 0,
                'exclamation_count': 0,
                'caps_ratio': 0,
                'punctuation_ratio': 0,
                'lexical_diversity': 0,
                'reading_ease': 0
            }
        
        # Basic counts
        word_count = len(text.split())
        char_count = len(text)
        sentence_count = len(re.findall(r'[.!?]+', text))
        
        # Word statistics
        words = text.split()
        avg_word_length = np.mean([len(word) for word in words]) if words else 0
        
        # Punctuation and formatting
        exclamation_count = text.count('!')
        caps_ratio = sum(1 for c in text if c.isupper()) / len(text) if text else 0
        punctuation_ratio = sum(1 for c in text if c in string.punctuation) / len(text) if text else 0
        
        # Lexical diversity (unique words / total words)
        unique_words = set(words)
        lexical_diversity = len(unique_words) / len(words) if words else 0
        
        # Reading ease (if text is long enough)
        try:
            reading_ease = flesch_reading_ease(text) if len(text) > 10 else 0
        except:
            reading_ease = 0
        
        return {
            'word_count': word_count,
            'char_count': char_count,
            'sentence_count': max(sentence_count, 1),  # At least 1 sentence
            'avg_word_length': avg_word_length,
            'exclamation_count': exclamation_count,
            'caps_ratio': caps_ratio,
            'punctuation_ratio': punctuation_ratio,
            'lexical_diversity': lexical_diversity,
            'reading_ease': reading_ease
        }
    
    def lemmatize_text(self, text):
        """Lemmatize text using spaCy"""
        if not self.nlp or pd.isna(text):
            return text
        
        doc = self.nlp(text)
        return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])
    
    def process_dataframe(self, df, text_column='review_text'):
        """Process entire dataframe"""
        print("üßπ Cleaning text...")
        df['clean_text'] = df[text_column].apply(self.clean_text)
        
        print("üìä Extracting text features...")
        text_features = df['clean_text'].apply(self.extract_text_features)
        text_features_df = pd.DataFrame(text_features.tolist())
        
        # Merge text features
        df = pd.concat([df, text_features_df], axis=1)
        
        if self.nlp:
            print("üî§ Lemmatizing text...")
            df['lemmatized_text'] = df['clean_text'].apply(self.lemmatize_text)
        
        return df

# Initialize preprocessor
preprocessor = TextPreprocessor(nlp)

# Process the dataframe
df_processed = preprocessor.process_dataframe(df.copy())

print("‚úÖ Text preprocessing completed!")
print(f"üìä New features added: {list(df_processed.columns[len(df.columns):])[:10]}...")## 2. Data Preprocessing and Text Cleaning# üîÑ PLACEHOLDER: Replace with your actual data loading
def load_data():
    """
    Load your e-commerce review dataset here.
    
    Replace this function with your actual data loading code:
    - df = pd.read_csv('your_dataset.csv')
    - df = pd.read_json('your_dataset.json')
    - df = load_from_database()
    """
    
    # For demonstration, we'll create a synthetic dataset
    print("üìä Creating synthetic dataset for demonstration...")
    
    np.random.seed(42)
    n_samples = 10000
    
    # Generate synthetic review data
    data = {
        'review_id': range(n_samples),
        'review_text': generate_synthetic_reviews(n_samples),
        'rating': np.random.choice([1, 2, 3, 4, 5], n_samples, p=[0.05, 0.1, 0.15, 0.35, 0.35]),
        'timestamp': pd.date_range('2020-01-01', periods=n_samples, freq='1H'),
        'user_id': [f'user_{i}' for i in np.random.randint(1, 5000, n_samples)],
        'product_id': [f'prod_{i}' for i in np.random.randint(1, 1000, n_samples)],
        'seller_id': [f'seller_{i}' for i in np.random.randint(1, 200, n_samples)],
        'verified_purchase': np.random.choice([True, False], n_samples, p=[0.8, 0.2]),
        'helpful_votes': np.random.poisson(2, n_samples)
    }
    
    df = pd.DataFrame(data)
    
    # Add some realistic patterns
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    return df

def generate_synthetic_reviews(n):
    """Generate synthetic review texts for demonstration"""
    positive_templates = [
        "Great product! Highly recommend it.",
        "Excellent quality and fast shipping.",
        "Love this item, exactly as described.",
        "Perfect! Will buy again.",
        "Amazing quality for the price."
    ]
    
    negative_templates = [
        "Poor quality, not as advertised.",
        "Waste of money, very disappointed.",
        "Broke after one use.",
        "Terrible product, don't buy.",
        "Not worth the price."
    ]
    
    fake_templates = [
        "Best product ever! 5 stars!!!",
        "AMAZING!!! BUY NOW!!!",
        "Perfect perfect perfect!",
        "Great great great!",
        "Excellent product A+++"
    ]
    
    all_templates = positive_templates + negative_templates + fake_templates
    return np.random.choice(all_templates, n)

# Load the data
df = load_data()
print(f"üìä Dataset loaded: {len(df)} reviews")
print(f"üìä Date range: {df['timestamp'].min()} to {df['timestamp'].max()}")
df.head()## 1. Data Loading and Initial Setup

**üìù Note for Researcher**: Replace this section with your actual data loading code. 

Expected columns:
- `review_text`: Review content
- `rating`: Numerical rating (1-5)
- `timestamp`: Review timestamp
- `user_id`: Unique user identifier
- `product_id`: Unique product identifier
- `seller_id`: Unique seller identifier
- `verified_purchase`: Boolean (optional)
- `helpful_votes`: Number of helpful votes (optional)
- `is_fake`: Target label for fake reviews (if available)
- `is_counterfeit`: Target label for counterfeit products (if available)# Download required NLTK data
nltk.download('vader_lexicon', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)

# Load spaCy model (run: python -m spacy download en_core_web_sm)
try:
    nlp = spacy.load("en_core_web_sm")
    print("‚úÖ spaCy model loaded successfully!")
except OSError:
    print("‚ö†Ô∏è Please install spaCy English model: python -m spacy download en_core_web_sm")
    nlp = None# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# Text processing
import re
import string
import spacy
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
from textstat import flesch_reading_ease, lexical_diversity

# Machine Learning
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
from sklearn.feature_selection import mutual_info_classif
import xgboost as xgb

# Deep Learning & Transformers
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification

# Explainability
import shap

# Time series and datetime
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Set random seed for reproducibility
np.random.seed(42)

print("‚úÖ All libraries imported successfully!")# The Link Between Fake Reviews and Intellectual Property Infringement: A Data-Driven Investigation

## Research Objectives:
1. **Identify key features** that distinguish fake reviews from genuine ones
2. **Analyze correlation** between fake review patterns and counterfeit product listings
3. **Develop predictive framework** to identify high-risk sellers/products associated with counterfeiting

---

## Pipeline Overview:
- **Data Preprocessing**: Text cleaning, tokenization, feature extraction
- **Sentiment Analysis**: VADER + Transformer-based sentiment scoring
- **Fake Review Labeling**: Heuristic-based weak supervision
- **Counterfeit Labeling**: Proxy indicators for IP infringement
- **Feature Engineering**: Comprehensive feature set creation
- **Modeling**: Classification models for fake reviews and counterfeit prediction
- **Analysis**: Correlation analysis and explainability
- **Visualization**: Interactive dashboards and plots